{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "import chromadb\n",
    "import markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama client\n",
    "ollama_client = ollama.Client()\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Set the path to your folder containing the files\n",
    "my_folder = \"../my_folder\"\n",
    "\n",
    "# Name of the database\n",
    "collection_name = \"md_embeddings\"\n",
    "\n",
    "# Create a collection in ChromaDB\n",
    "try:\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "    print(f\"Collection '{collection_name}' already exists.\")\n",
    "except ValueError:\n",
    "    collection = chroma_client.create_collection(name=collection_name)\n",
    "    print(f\"Created new collection '{collection_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove collection\n",
    "#chroma_client.delete_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_markdown_files(directory):\n",
    "    content_list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".md\") or filename.endswith(\".qmd\"):\n",
    "            with open(os.path.join(directory, filename), 'r') as file:\n",
    "                content = file.read()\n",
    "                content_list.append((filename, content))\n",
    "    return content_list\n",
    "\n",
    "def generate_embedding(text):\n",
    "    # Generate embedding using Ollama\n",
    "    response = ollama_client.embeddings(model=\"llama3\", prompt=text)\n",
    "    embedding = response['embedding']\n",
    "    return embedding\n",
    "\n",
    "def store_vectors_in_chromadb(vectors, metadata, chroma_collection):\n",
    "    for vector, meta in zip(vectors, metadata):\n",
    "        chroma_collection.add(\n",
    "            embeddings=[vector],\n",
    "            documents=[meta['content']],\n",
    "            ids=[meta['filename']]\n",
    "        )\n",
    "\n",
    "def find_closest_files(query_text, chroma_collection, top_n=5):\n",
    "    # Step 1: Generate the embedding for the query content\n",
    "    query_embedding = generate_embedding(query_text)\n",
    "\n",
    "    # Step 2: Query ChromaDB with the generated embedding\n",
    "    results = chroma_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_n\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_vectors_in_chromadb(vectors, metadata, chroma_collection):\n",
    "    for vector, meta in zip(vectors, metadata):\n",
    "        vector_id = meta['filename']\n",
    "        \n",
    "        # Check if the ID already exists\n",
    "        try:\n",
    "            # Retrieve the existing document\n",
    "            existing_document = chroma_collection.get(ids=[vector_id])\n",
    "            if existing_document:\n",
    "                # Update existing vector\n",
    "                chroma_collection.update(\n",
    "                    embeddings=[vector],\n",
    "                    documents=[meta['content']],\n",
    "                    ids=[vector_id]\n",
    "                )\n",
    "            else:\n",
    "                # Add new vector\n",
    "                chroma_collection.add(\n",
    "                    embeddings=[vector],\n",
    "                    documents=[meta['content']],\n",
    "                    ids=[vector_id]\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error storing vector: {e}\")\n",
    "\n",
    "def retrieve_context_from_chromadb(query_text, chroma_collection, top_n=5):\n",
    "    # Generate the embedding for the query content\n",
    "    query_embedding = generate_embedding(query_text)\n",
    "\n",
    "    # Query ChromaDB with the generated embedding\n",
    "    results = chroma_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_n\n",
    "    )\n",
    "\n",
    "    # Extract documents and their IDs from the results\n",
    "    document_ids = results.get('ids', [[]])[0]  # Extract the list of IDs\n",
    "    documents = results.get('documents', [[]])[0]  # Extract the list of documents\n",
    "    context = \" \".join(f\"[{doc_id}] {doc}\" for doc_id, doc in zip(document_ids, documents))\n",
    "    return context\n",
    "\n",
    "def get_system_message_rag(content):\n",
    "    return f\"\"\"You are an expert consultant helping executive advisors to get relevant information from internal documents.\n",
    "\n",
    "    Generate your response by following the steps below:\n",
    "    1. Recursively break down the question into smaller questions.\n",
    "    2. For each question/directive:\n",
    "        2a. Select the most relevant information from the context in light of the conversation history.\n",
    "    3. Generate a draft response using selected information.\n",
    "    4. Remove duplicate content from draft response.\n",
    "    5. Generate your final response after adjusting it to increase accuracy and relevance.\n",
    "    6. Do not try to summarise the answers, explain it properly.\n",
    "    6. Only show your final response! \n",
    "    \n",
    "    Constraints:\n",
    "    1. DO NOT PROVIDE ANY EXPLANATION OR DETAILS OR MENTION THAT YOU WERE GIVEN CONTEXT.\n",
    "    2. Don't mention that you are not able to find the answer in the provided context.\n",
    "    3. Don't make up the answers by yourself.\n",
    "    4. Try your best to provide answer from the given context.\n",
    "\n",
    "    CONTENT:\n",
    "    {content}\n",
    "    \"\"\"\n",
    "\n",
    "def get_ques_response_prompt(question):\n",
    "    return f\"\"\"\n",
    "    ==============================================================\n",
    "    Based on the above context, please provide the answer to the following question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "\n",
    "def generate_rag_response(content, question, model=\"llama3\"):\n",
    "    stream = ollama_client.chat(model=model, messages=[\n",
    "        {\"role\": \"system\", \"content\": get_system_message_rag(content)},            \n",
    "        {\"role\": \"user\", \"content\": get_ques_response_prompt(question)}\n",
    "    ], stream=True)\n",
    "\n",
    "    print(get_system_message_rag(content))\n",
    "    print(get_ques_response_prompt(question))\n",
    "    print(\"####### THINKING OF ANSWER............ \")\n",
    "    full_answer = ''\n",
    "    for chunk in stream:\n",
    "        print(chunk['message']['content'], end='', flush=True)\n",
    "        full_answer = ''.join([full_answer, chunk['message']['content']])\n",
    "\n",
    "    return full_answer\n",
    "\n",
    "def main():\n",
    "    # Step 1: Read and parse QMD/MD files\n",
    "    markdown_directory = my_folder\n",
    "    markdown_files_content = read_markdown_files(markdown_directory)\n",
    "    \n",
    "    # Step 2: Vectorize content using Ollama\n",
    "    vectors = []\n",
    "    metadata = []\n",
    "    for filename, content in markdown_files_content:\n",
    "        embedding = generate_embedding(content)\n",
    "        vectors.append(embedding)\n",
    "        metadata.append({'filename': filename, 'content': content})\n",
    "    \n",
    "    # Step 3: Store vectors in ChromaDB\n",
    "    store_vectors_in_chromadb(vectors, metadata, collection)\n",
    "\n",
    "    # Step 4: Retrieve information and generate a response\n",
    "    query_text = \"What is the commonly referenced measure of central tendency?\"\n",
    "    context = retrieve_context_from_chromadb(query_text, collection)\n",
    "    response = generate_rag_response(context, query_text)\n",
    "    \n",
    "    print(\"Generated Response:\")\n",
    "    print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Step 1: Read and parse QMD/MD files\n",
    "    markdown_directory = my_folder\n",
    "    markdown_files_content = read_markdown_files(markdown_directory)\n",
    "    \n",
    "    # Step 2: Vectorize content using Ollama\n",
    "    vectors = []\n",
    "    metadata = []\n",
    "    for filename, content in markdown_files_content:\n",
    "        embedding = generate_embedding(content)\n",
    "        vectors.append(embedding)\n",
    "        metadata.append({'filename': filename, 'content': content})\n",
    "    \n",
    "    # Step 3: Store vectors in ChromaDB\n",
    "    store_vectors_in_chromadb(vectors, metadata, collection)\n",
    "\n",
    "    while True:\n",
    "        query_text = input(\"Enter your query (or 'exit' to quit): \")\n",
    "        if query_text.lower() == 'exit':\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        \n",
    "        # Retrieve context from ChromaDB\n",
    "        context = retrieve_context_from_chromadb(query_text, collection)\n",
    "        \n",
    "        # Generate response with context\n",
    "        response = generate_rag_response(context, query_text)\n",
    "        \n",
    "        # Print response\n",
    "        print(\"\\nGenerated Response:\")\n",
    "        print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
